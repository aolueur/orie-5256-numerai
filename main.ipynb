{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORIE 5256 Numerai Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available versions:\n",
      " ['v5.0']\n",
      "Available v5.0 files:\n",
      " ['v5.0/features.json', 'v5.0/live.parquet', 'v5.0/live_benchmark_models.parquet', 'v5.0/live_example_preds.csv', 'v5.0/live_example_preds.parquet', 'v5.0/meta_model.parquet', 'v5.0/train.parquet', 'v5.0/train_benchmark_models.parquet', 'v5.0/validation.parquet', 'v5.0/validation_benchmark_models.parquet', 'v5.0/validation_example_preds.csv', 'v5.0/validation_example_preds.parquet']\n"
     ]
    }
   ],
   "source": [
    "# Initialize NumerAPI - the official Python API client for Numerai\n",
    "from numerapi import NumerAPI\n",
    "napi = NumerAPI()\n",
    "\n",
    "# list the datasets and available versions\n",
    "all_datasets = napi.list_datasets()\n",
    "dataset_versions = list(set(d.split('/')[0] for d in all_datasets))\n",
    "print(\"Available versions:\\n\", dataset_versions)\n",
    "\n",
    "# Set data version to one of the latest datasets\n",
    "DATA_VERSION = \"v5.0\"\n",
    "\n",
    "# Print all files available for download for our version\n",
    "current_version_files = [f for f in all_datasets if f.startswith(DATA_VERSION)]\n",
    "print(\"Available\", DATA_VERSION, \"files:\\n\", current_version_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `medium` feature set offer by Numerai. This feature set contains a total of 705 features. In this section, we will perform some feature engineering methods to ensure the stationarity of the data, and to reduce the dimensionality to avoid curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 23:54:55,382 INFO numerapi.utils: target file already exists\n",
      "2024-11-27 23:54:55,383 INFO numerapi.utils: download complete\n",
      "2024-11-27 23:54:56,071 INFO numerapi.utils: target file already exists\n",
      "2024-11-27 23:54:56,073 INFO numerapi.utils: download complete\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "napi = NumerAPI()  # initialize API client\n",
    "DATA_VERSION = 'v5.0'\n",
    "\n",
    "# Load metadata\n",
    "napi.download_dataset(f'{DATA_VERSION}/features.json')\n",
    "feature_metadata = json.load(open(f'{DATA_VERSION}/features.json'))\n",
    "feature_sets = feature_metadata['feature_sets']\n",
    "medium_features = feature_sets['medium']\n",
    "\n",
    "# Load training data\n",
    "napi.download_dataset(f'{DATA_VERSION}/train.parquet')\n",
    "train_set = pd.read_parquet(f'{DATA_VERSION}/train.parquet', columns=['era', 'target'] + medium_features)\n",
    "\n",
    "# Downsample to every 4th era\n",
    "train_set = train_set[train_set['era'].isin(train_set['era'].unique()[::4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>era</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_able_deprived_nona</th>\n",
       "      <th>feature_ablest_inflexional_egeria</th>\n",
       "      <th>feature_absorbable_hyperalgesic_mode</th>\n",
       "      <th>feature_accoutered_revolute_vexillology</th>\n",
       "      <th>feature_acetose_crackerjack_needlecraft</th>\n",
       "      <th>feature_acheulian_conserving_output</th>\n",
       "      <th>feature_acronychal_bilobate_stevenage</th>\n",
       "      <th>feature_acrylic_gallic_wine</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_working_jain_acromegaly</th>\n",
       "      <th>feature_wrapround_chrestomathic_timarau</th>\n",
       "      <th>feature_xanthic_transpadane_saleswoman</th>\n",
       "      <th>feature_xanthochroid_petrified_gutenberg</th>\n",
       "      <th>feature_zincy_cirrhotic_josh</th>\n",
       "      <th>feature_zippy_trine_diffraction</th>\n",
       "      <th>feature_zonal_snuffly_chemism</th>\n",
       "      <th>feature_zygotic_middlebrow_caribbean</th>\n",
       "      <th>feature_zymolytic_intertidal_privet</th>\n",
       "      <th>feature_zymotic_windswept_cooky</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n0007b5abb0c3a25</th>\n",
       "      <td>0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n003bba8a98662e4</th>\n",
       "      <td>0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n003bee128c2fcfc</th>\n",
       "      <td>0001</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n0048ac83aff7194</th>\n",
       "      <td>0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n0055a2401ba6480</th>\n",
       "      <td>0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 707 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   era  target  feature_able_deprived_nona  \\\n",
       "id                                                           \n",
       "n0007b5abb0c3a25  0001    0.25                           1   \n",
       "n003bba8a98662e4  0001    0.25                           3   \n",
       "n003bee128c2fcfc  0001    0.75                           1   \n",
       "n0048ac83aff7194  0001    0.25                           1   \n",
       "n0055a2401ba6480  0001    0.25                           3   \n",
       "\n",
       "                  feature_ablest_inflexional_egeria  \\\n",
       "id                                                    \n",
       "n0007b5abb0c3a25                                  2   \n",
       "n003bba8a98662e4                                  2   \n",
       "n003bee128c2fcfc                                  2   \n",
       "n0048ac83aff7194                                  2   \n",
       "n0055a2401ba6480                                  2   \n",
       "\n",
       "                  feature_absorbable_hyperalgesic_mode  \\\n",
       "id                                                       \n",
       "n0007b5abb0c3a25                                     3   \n",
       "n003bba8a98662e4                                     4   \n",
       "n003bee128c2fcfc                                     0   \n",
       "n0048ac83aff7194                                     3   \n",
       "n0055a2401ba6480                                     3   \n",
       "\n",
       "                  feature_accoutered_revolute_vexillology  \\\n",
       "id                                                          \n",
       "n0007b5abb0c3a25                                        2   \n",
       "n003bba8a98662e4                                        1   \n",
       "n003bee128c2fcfc                                        2   \n",
       "n0048ac83aff7194                                        4   \n",
       "n0055a2401ba6480                                        4   \n",
       "\n",
       "                  feature_acetose_crackerjack_needlecraft  \\\n",
       "id                                                          \n",
       "n0007b5abb0c3a25                                        3   \n",
       "n003bba8a98662e4                                        0   \n",
       "n003bee128c2fcfc                                        4   \n",
       "n0048ac83aff7194                                        0   \n",
       "n0055a2401ba6480                                        1   \n",
       "\n",
       "                  feature_acheulian_conserving_output  \\\n",
       "id                                                      \n",
       "n0007b5abb0c3a25                                    2   \n",
       "n003bba8a98662e4                                    2   \n",
       "n003bee128c2fcfc                                    2   \n",
       "n0048ac83aff7194                                    2   \n",
       "n0055a2401ba6480                                    2   \n",
       "\n",
       "                  feature_acronychal_bilobate_stevenage  \\\n",
       "id                                                        \n",
       "n0007b5abb0c3a25                                      2   \n",
       "n003bba8a98662e4                                      3   \n",
       "n003bee128c2fcfc                                      0   \n",
       "n0048ac83aff7194                                      3   \n",
       "n0055a2401ba6480                                      4   \n",
       "\n",
       "                  feature_acrylic_gallic_wine  ...  \\\n",
       "id                                             ...   \n",
       "n0007b5abb0c3a25                            2  ...   \n",
       "n003bba8a98662e4                            2  ...   \n",
       "n003bee128c2fcfc                            2  ...   \n",
       "n0048ac83aff7194                            2  ...   \n",
       "n0055a2401ba6480                            2  ...   \n",
       "\n",
       "                  feature_working_jain_acromegaly  \\\n",
       "id                                                  \n",
       "n0007b5abb0c3a25                                2   \n",
       "n003bba8a98662e4                                2   \n",
       "n003bee128c2fcfc                                2   \n",
       "n0048ac83aff7194                                2   \n",
       "n0055a2401ba6480                                2   \n",
       "\n",
       "                  feature_wrapround_chrestomathic_timarau  \\\n",
       "id                                                          \n",
       "n0007b5abb0c3a25                                        0   \n",
       "n003bba8a98662e4                                        0   \n",
       "n003bee128c2fcfc                                        3   \n",
       "n0048ac83aff7194                                        0   \n",
       "n0055a2401ba6480                                        1   \n",
       "\n",
       "                  feature_xanthic_transpadane_saleswoman  \\\n",
       "id                                                         \n",
       "n0007b5abb0c3a25                                       3   \n",
       "n003bba8a98662e4                                       0   \n",
       "n003bee128c2fcfc                                       3   \n",
       "n0048ac83aff7194                                       2   \n",
       "n0055a2401ba6480                                       3   \n",
       "\n",
       "                  feature_xanthochroid_petrified_gutenberg  \\\n",
       "id                                                           \n",
       "n0007b5abb0c3a25                                         2   \n",
       "n003bba8a98662e4                                         2   \n",
       "n003bee128c2fcfc                                         2   \n",
       "n0048ac83aff7194                                         1   \n",
       "n0055a2401ba6480                                         3   \n",
       "\n",
       "                  feature_zincy_cirrhotic_josh  \\\n",
       "id                                               \n",
       "n0007b5abb0c3a25                             4   \n",
       "n003bba8a98662e4                             0   \n",
       "n003bee128c2fcfc                             2   \n",
       "n0048ac83aff7194                             1   \n",
       "n0055a2401ba6480                             2   \n",
       "\n",
       "                  feature_zippy_trine_diffraction  \\\n",
       "id                                                  \n",
       "n0007b5abb0c3a25                                3   \n",
       "n003bba8a98662e4                                0   \n",
       "n003bee128c2fcfc                                3   \n",
       "n0048ac83aff7194                                4   \n",
       "n0055a2401ba6480                                4   \n",
       "\n",
       "                  feature_zonal_snuffly_chemism  \\\n",
       "id                                                \n",
       "n0007b5abb0c3a25                              2   \n",
       "n003bba8a98662e4                              2   \n",
       "n003bee128c2fcfc                              2   \n",
       "n0048ac83aff7194                              2   \n",
       "n0055a2401ba6480                              2   \n",
       "\n",
       "                  feature_zygotic_middlebrow_caribbean  \\\n",
       "id                                                       \n",
       "n0007b5abb0c3a25                                     1   \n",
       "n003bba8a98662e4                                     0   \n",
       "n003bee128c2fcfc                                     2   \n",
       "n0048ac83aff7194                                     0   \n",
       "n0055a2401ba6480                                     4   \n",
       "\n",
       "                  feature_zymolytic_intertidal_privet  \\\n",
       "id                                                      \n",
       "n0007b5abb0c3a25                                    0   \n",
       "n003bba8a98662e4                                    0   \n",
       "n003bee128c2fcfc                                    2   \n",
       "n0048ac83aff7194                                    2   \n",
       "n0055a2401ba6480                                    1   \n",
       "\n",
       "                  feature_zymotic_windswept_cooky  \n",
       "id                                                 \n",
       "n0007b5abb0c3a25                                0  \n",
       "n003bba8a98662e4                                0  \n",
       "n003bee128c2fcfc                                4  \n",
       "n0048ac83aff7194                                1  \n",
       "n0055a2401ba6480                                3  \n",
       "\n",
       "[5 rows x 707 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Low Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we filter out those features that are highly correlated with each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pairwise correlations between features. Drop one from each highly correlated pari (threshold = .8)\n",
    "\n",
    "# correlation_matrix = train_set[medium_features].corr().abs()\n",
    "# upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "# train_set.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set.to_parquet(f'train_set_low_corr.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = pd.read_parquet('train_set_low_corr.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store ne wfeatures\n",
    "# low_corr_features = list(train_set.columns[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(low_corr_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Principal Component Analysis (PCA) to reduce the dimensionality of the data. The first 100 principal components will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to the features and store the first 100 components\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=.95)\n",
    "# fit PCA to the features\n",
    "pca_X = pca.fit_transform(train_set[medium_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the PCA features in the training set\n",
    "num_pca_features = pca_X.shape[1]\n",
    "pca_features = [f'pca_{i}' for i in range(num_pca_features)]  # name of the pca features\n",
    "df_pca_features = pd.DataFrame(pca_X, index=train_set.index, columns=pca_features)\n",
    "train_set = pd.concat([train_set, df_pca_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>era</th>\n",
       "      <th>target</th>\n",
       "      <th>pca_0</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>pca_4</th>\n",
       "      <th>pca_5</th>\n",
       "      <th>pca_6</th>\n",
       "      <th>pca_7</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_302</th>\n",
       "      <th>pca_303</th>\n",
       "      <th>pca_304</th>\n",
       "      <th>pca_305</th>\n",
       "      <th>pca_306</th>\n",
       "      <th>pca_307</th>\n",
       "      <th>pca_308</th>\n",
       "      <th>pca_309</th>\n",
       "      <th>pca_310</th>\n",
       "      <th>pca_311</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n0007b5abb0c3a25</th>\n",
       "      <td>0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-9.288973</td>\n",
       "      <td>3.066574</td>\n",
       "      <td>-3.821182</td>\n",
       "      <td>3.503268</td>\n",
       "      <td>3.933269</td>\n",
       "      <td>7.436123</td>\n",
       "      <td>4.061404</td>\n",
       "      <td>-2.971538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.808702</td>\n",
       "      <td>1.037759</td>\n",
       "      <td>-0.650644</td>\n",
       "      <td>-1.411315</td>\n",
       "      <td>-0.522225</td>\n",
       "      <td>1.117435</td>\n",
       "      <td>0.255024</td>\n",
       "      <td>0.073524</td>\n",
       "      <td>0.838775</td>\n",
       "      <td>-0.226892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n003bba8a98662e4</th>\n",
       "      <td>0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-8.709131</td>\n",
       "      <td>-8.615216</td>\n",
       "      <td>-1.399786</td>\n",
       "      <td>6.590700</td>\n",
       "      <td>0.165589</td>\n",
       "      <td>1.353093</td>\n",
       "      <td>-0.327783</td>\n",
       "      <td>-3.305413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.382571</td>\n",
       "      <td>-0.123572</td>\n",
       "      <td>0.725335</td>\n",
       "      <td>1.030723</td>\n",
       "      <td>-1.313305</td>\n",
       "      <td>0.207756</td>\n",
       "      <td>0.014860</td>\n",
       "      <td>-1.004689</td>\n",
       "      <td>-2.195562</td>\n",
       "      <td>-0.734417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n003bee128c2fcfc</th>\n",
       "      <td>0001</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.232459</td>\n",
       "      <td>10.493623</td>\n",
       "      <td>-6.811598</td>\n",
       "      <td>-1.677351</td>\n",
       "      <td>-1.198478</td>\n",
       "      <td>-0.748976</td>\n",
       "      <td>1.672055</td>\n",
       "      <td>1.551992</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.520323</td>\n",
       "      <td>-0.056187</td>\n",
       "      <td>-0.871155</td>\n",
       "      <td>-0.420036</td>\n",
       "      <td>0.723029</td>\n",
       "      <td>-0.071916</td>\n",
       "      <td>-0.083538</td>\n",
       "      <td>0.067414</td>\n",
       "      <td>-0.597459</td>\n",
       "      <td>0.102226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n0048ac83aff7194</th>\n",
       "      <td>0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.826326</td>\n",
       "      <td>-13.853773</td>\n",
       "      <td>-0.894217</td>\n",
       "      <td>-6.760077</td>\n",
       "      <td>0.588427</td>\n",
       "      <td>6.413077</td>\n",
       "      <td>2.290896</td>\n",
       "      <td>6.145121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215866</td>\n",
       "      <td>-0.467865</td>\n",
       "      <td>-0.701282</td>\n",
       "      <td>0.439762</td>\n",
       "      <td>0.014501</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>0.726732</td>\n",
       "      <td>0.381191</td>\n",
       "      <td>0.428412</td>\n",
       "      <td>-0.114618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n0055a2401ba6480</th>\n",
       "      <td>0001</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-4.024114</td>\n",
       "      <td>-5.980213</td>\n",
       "      <td>-1.302330</td>\n",
       "      <td>-1.849217</td>\n",
       "      <td>-5.212901</td>\n",
       "      <td>0.188909</td>\n",
       "      <td>-0.113856</td>\n",
       "      <td>5.412827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017906</td>\n",
       "      <td>0.027337</td>\n",
       "      <td>0.639182</td>\n",
       "      <td>0.313109</td>\n",
       "      <td>0.246711</td>\n",
       "      <td>0.251916</td>\n",
       "      <td>0.505785</td>\n",
       "      <td>0.107952</td>\n",
       "      <td>0.278314</td>\n",
       "      <td>0.390047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 314 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   era  target     pca_0      pca_1     pca_2     pca_3  \\\n",
       "id                                                                        \n",
       "n0007b5abb0c3a25  0001    0.25 -9.288973   3.066574 -3.821182  3.503268   \n",
       "n003bba8a98662e4  0001    0.25 -8.709131  -8.615216 -1.399786  6.590700   \n",
       "n003bee128c2fcfc  0001    0.75 -0.232459  10.493623 -6.811598 -1.677351   \n",
       "n0048ac83aff7194  0001    0.25  1.826326 -13.853773 -0.894217 -6.760077   \n",
       "n0055a2401ba6480  0001    0.25 -4.024114  -5.980213 -1.302330 -1.849217   \n",
       "\n",
       "                     pca_4     pca_5     pca_6     pca_7  ...   pca_302  \\\n",
       "id                                                        ...             \n",
       "n0007b5abb0c3a25  3.933269  7.436123  4.061404 -2.971538  ... -0.808702   \n",
       "n003bba8a98662e4  0.165589  1.353093 -0.327783 -3.305413  ... -0.382571   \n",
       "n003bee128c2fcfc -1.198478 -0.748976  1.672055  1.551992  ... -0.520323   \n",
       "n0048ac83aff7194  0.588427  6.413077  2.290896  6.145121  ...  0.215866   \n",
       "n0055a2401ba6480 -5.212901  0.188909 -0.113856  5.412827  ...  0.017906   \n",
       "\n",
       "                   pca_303   pca_304   pca_305   pca_306   pca_307   pca_308  \\\n",
       "id                                                                             \n",
       "n0007b5abb0c3a25  1.037759 -0.650644 -1.411315 -0.522225  1.117435  0.255024   \n",
       "n003bba8a98662e4 -0.123572  0.725335  1.030723 -1.313305  0.207756  0.014860   \n",
       "n003bee128c2fcfc -0.056187 -0.871155 -0.420036  0.723029 -0.071916 -0.083538   \n",
       "n0048ac83aff7194 -0.467865 -0.701282  0.439762  0.014501 -0.085253  0.726732   \n",
       "n0055a2401ba6480  0.027337  0.639182  0.313109  0.246711  0.251916  0.505785   \n",
       "\n",
       "                   pca_309   pca_310   pca_311  \n",
       "id                                              \n",
       "n0007b5abb0c3a25  0.073524  0.838775 -0.226892  \n",
       "n003bba8a98662e4 -1.004689 -2.195562 -0.734417  \n",
       "n003bee128c2fcfc  0.067414 -0.597459  0.102226  \n",
       "n0048ac83aff7194  0.381191  0.428412 -0.114618  \n",
       "n0055a2401ba6480  0.107952  0.278314  0.390047  \n",
       "\n",
       "[5 rows x 314 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the original features\n",
    "train_set.drop(columns=medium_features, inplace=True)\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Mean Decrease Accuracy (MDA) analysis to select the most important features. For this multi-class classification problem, our baseline classifier is Random Forest. We will use Purged K-Fold Cross Validation with AUC-ROC as scoring metric. Features with positive mean score improvement will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['era'] = train_set['era'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct inputs\n",
    "\n",
    "t1 = pd.Series((train_set['era'] + 4).values, index=train_set['era'])\n",
    "X = train_set[pca_features].copy()\n",
    "X.index = t1.index\n",
    "y = train_set['target'].copy()\n",
    "y.index = t1.index\n",
    "y = y.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sample weights\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "sample_weight = compute_sample_weight(class_weight='balanced', y=train_set['target'])\n",
    "sample_weight = pd.Series(sample_weight, index=train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection._split import _BaseKFold\n",
    "\n",
    "class PurgedKFold(_BaseKFold):\n",
    "    \"\"\"Extend KFold class to work with labels that span intervals.\n",
    "\n",
    "    The train is purged of observations overlapping test-label intervals.\n",
    "    Test set is assumed contiguous (shuffle=False), w/o training samples in between.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=3, t1=None, pctEmbargo=0.0):\n",
    "        \"\"\"Initialize PurgedKFold object.\n",
    "\n",
    "        Args:\n",
    "            n_splits (int): Number of splits. Default is 3.\n",
    "            t1 (pd.Series): \n",
    "                t1.index: time when the observation started\n",
    "                t1.value: time when the observation ended\n",
    "            pctEmbargo (float): Percentage of embargo on test set. Embargo step = pctEmbargo * T. Default is 0.\n",
    "        \"\"\"\n",
    "        if not isinstance(t1, pd.Series):\n",
    "            raise ValueError('Label Through Dates must be a pd.Series')\n",
    "        super(PurgedKFold, self).__init__(\n",
    "            n_splits, shufï¬‚e=False, random_state=None\n",
    "        )\n",
    "\n",
    "        self.t1 = t1\n",
    "        self.pctEmbargo = pctEmbargo\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (pd.Series): Labels.\n",
    "            groups: Ignored.\n",
    "        \"\"\"\n",
    "        if (X.index == self.t1.index).sum() != len(self.t1):\n",
    "            raise ValueError('X and ThruDateValues must have the same index')\n",
    "\n",
    "        indices = np.arange(X.shape[0])\n",
    "\n",
    "        mbrg = int(X.shape[0] * self.pctEmbargo)\n",
    "        test_starts = [\n",
    "            (i[0], i[-1] + 1)\n",
    "            for i in np.array_split(np.arange(X.shape[0]), self.n_splits)\n",
    "        ]\n",
    "        for test_start, test_end in test_starts:\n",
    "            t0 = self.t1.index[test_start]   # start of test set\n",
    "            test_indices = indices[test_start: test_end]\n",
    "\n",
    "            max_t1 = self.t1.iloc[test_indices].max()\n",
    "            maxT1Idx = self.t1.index.searchsorted(self.t1.iloc[test_indices].max())\n",
    "            train_indices = list(t1[t1 <= t0].reset_index(drop=True).index)\n",
    "            if maxT1Idx < X.shape[0]:   # right train (with embargo)\n",
    "                train_indices = np.concatenate(\n",
    "                    (train_indices, indices[maxT1Idx + mbrg :])\n",
    "                )\n",
    "            yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featImpMDA(\n",
    "    clf, X, y, cv, sample_weight, t1, pctEmbargo, scoring='auc-roc'\n",
    "):\n",
    "    \"\"\"feat importance based on OOS score reduction\"\"\"\n",
    "    if scoring not in ['auc-roc']:\n",
    "        raise Exception('wrong scoring method.')\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    cvGen = PurgedKFold(\n",
    "        n_splits=cv, t1=t1, pctEmbargo=pctEmbargo\n",
    "    )   # purged cv\n",
    "    scr0 = pd.Series()\n",
    "    scr1 = pd.DataFrame(columns=X.columns)\n",
    "\n",
    "    for i, (train, test) in enumerate(cvGen.split(X=X)):\n",
    "        X0, y0, w0 = X.iloc[train, :], y.iloc[train], sample_weight.iloc[train]\n",
    "        X1, y1, w1 = X.iloc[test, :], y.iloc[test], sample_weight.iloc[test]\n",
    "        fit = clf.fit(X=X0, y=y0, sample_weight=w0.values)\n",
    "        if scoring == 'auc-roc':\n",
    "            prob = fit.predict_proba(X1)\n",
    "            scr0.loc[i] = roc_auc_score(\n",
    "                y1, prob, sample_weight=w1.values, labels=clf.classes_, multi_class='ovr', average='macro'\n",
    "            )\n",
    "        else:\n",
    "            raise Exception('Only auc-roc scoring is supported')\n",
    "        for j in X.columns:\n",
    "            X1_ = X1.copy(deep=True)\n",
    "            np.random.shuffle(X1_[j].values)   # permutation of a single column\n",
    "            if scoring == 'auc-roc':\n",
    "                prob = fit.predict_proba(X1_)\n",
    "                scr1.loc[i, j] = roc_auc_score(\n",
    "                    y1, prob, sample_weight=w1.values, labels=clf.classes_, multi_class='ovr', average='macro'\n",
    "                )\n",
    "            else:\n",
    "                raise Exception('Only auc-roc scoring is supported')\n",
    "    imp = (-scr1).add(scr0, axis=0)\n",
    "    if scoring == 'auc-roc':\n",
    "        imp = imp / (1.0 - scr1)\n",
    "    else:\n",
    "        raise Exception('Only auc-roc scoring is supported')\n",
    "    imp = pd.concat(\n",
    "        {'mean': imp.mean(), 'std': imp.std() * imp.shape[0] ** -0.5}, axis=1\n",
    "    )\n",
    "    return imp, scr0.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      3\u001b[0m clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m imp, scr0mean \u001b[38;5;241m=\u001b[39m \u001b[43mfeatImpMDA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpctEmbargo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m, in \u001b[0;36mfeatImpMDA\u001b[0;34m(clf, X, y, cv, sample_weight, t1, pctEmbargo, scoring)\u001b[0m\n\u001b[1;32m     28\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(X1_[j]\u001b[38;5;241m.\u001b[39mvalues)   \u001b[38;5;66;03m# permutation of a single column\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scoring \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc-roc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     scr1\u001b[38;5;241m.\u001b[39mloc[i, j] \u001b[38;5;241m=\u001b[39m roc_auc_score(\n\u001b[1;32m     32\u001b[0m         y1, prob, sample_weight\u001b[38;5;241m=\u001b[39mw1\u001b[38;5;241m.\u001b[39mvalues, labels\u001b[38;5;241m=\u001b[39mclf\u001b[38;5;241m.\u001b[39mclasses_, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/orie5256_numerai/.conda/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:957\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    952\u001b[0m all_proba \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    953\u001b[0m     np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], j), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[1;32m    955\u001b[0m ]\n\u001b[1;32m    956\u001b[0m lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[0;32m--> 957\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msharedmem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accumulate_prediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proba \u001b[38;5;129;01min\u001b[39;00m all_proba:\n\u001b[1;32m    963\u001b[0m     proba \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_)\n",
      "File \u001b[0;32m~/orie5256_numerai/.conda/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/orie5256_numerai/.conda/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/orie5256_numerai/.conda/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/orie5256_numerai/.conda/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_features=int(1))\n",
    "imp, scr0mean = featImpMDA(\n",
    "    clf,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=5,\n",
    "    sample_weight=sample_weight,\n",
    "    t1=t1,\n",
    "    pctEmbargo=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features with import mean > 0\n",
    "imp_pca_features = list(imp[imp['mean'] > 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_selected = train_set[['era', 'target'] + imp_pca_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_selected.to_parquet('train_set_selected.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We will use the LightGBM model to train our model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bs/ldd4wphx6yl7m8qmvfpzfl440000gn/T/ipykernel_22909/4222370160.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_set_selected['target'] = le.fit_transform(train_set_selected['target'].astype(str))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 79560\n",
      "[LightGBM] [Info] Number of data points in the train set: 688184, number of used features: 312\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "clf_lgbm = LGBMClassifier(\n",
    "    n_estimators=1024,\n",
    "    learning_rate=.01,\n",
    "    max_depth=10,\n",
    "    num_leaves=2**5-1,\n",
    "    colsample_bytree=.1,\n",
    "    min_data_in_leaf=10000,\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_set_selected['target'] = le.fit_transform(train_set_selected['target'].astype(str))\n",
    "\n",
    "\n",
    "fit = clf_lgbm.fit(train_set[pca_features], train_set['target'].astype(str), sample_weight=sample_weight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10000, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10000\n",
      "ROC AUC Score: 0.6539346691733892\n"
     ]
    }
   ],
   "source": [
    "model = fit\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get predicted probabilities on the training set\n",
    "train_probs = model.predict_proba(train_set[pca_features])\n",
    "\n",
    "# Encode string labels to integers if necessary\n",
    "true_labels = train_set['target'].astype(str)\n",
    "\n",
    "# Compute the ROC AUC score\n",
    "train_score = roc_auc_score(true_labels, train_probs, multi_class='ovr', average='macro')\n",
    "print(f\"ROC AUC Score: {train_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(true_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
